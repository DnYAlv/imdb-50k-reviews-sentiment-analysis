{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Convert to input for GCN","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews-clean/clean_reviews.csv')","metadata":{"execution":{"iopub.status.busy":"2022-12-24T04:11:10.718691Z","iopub.execute_input":"2022-12-24T04:11:10.719071Z","iopub.status.idle":"2022-12-24T04:11:11.639079Z","shell.execute_reply.started":"2022-12-24T04:11:10.719040Z","shell.execute_reply":"2022-12-24T04:11:11.638178Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-24T03:23:06.184580Z","iopub.execute_input":"2022-12-24T03:23:06.184954Z","iopub.status.idle":"2022-12-24T03:23:06.210090Z","shell.execute_reply.started":"2022-12-24T03:23:06.184917Z","shell.execute_reply":"2022-12-24T03:23:06.208994Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                       clean_reviews sentiment\n0  one reviewers mentioned watching 1 oz episode ...  positive\n1  wonderful little production filming technique ...  positive\n2  thought wonderful way spend time hot summer we...  positive\n3  basically family little boy jake thinks zombie...  negative\n4  mattei love time money visually stunning film ...  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>clean_reviews</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>one reviewers mentioned watching 1 oz episode ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>wonderful little production filming technique ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>thought wonderful way spend time hot summer we...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>basically family little boy jake thinks zombie...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>mattei love time money visually stunning film ...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"list(df['clean_reviews'])[:2]","metadata":{"execution":{"iopub.status.busy":"2022-12-24T03:23:06.211693Z","iopub.execute_input":"2022-12-24T03:23:06.212081Z","iopub.status.idle":"2022-12-24T03:23:06.232893Z","shell.execute_reply.started":"2022-12-24T03:23:06.212015Z","shell.execute_reply":"2022-12-24T03:23:06.231824Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"['one reviewers mentioned watching 1 oz episode hooked right exactly happened first thing struck oz brutality unflinching scenes violence set right word go trust show faint hearted timid show pulls punches regards drugs sex violence hardcore classic use word called oz nickname given oswald maximum security state focuses mainly emerald city experimental section prison cells glass fronts face privacy high agenda city home many aryans muslims latinos christians italians irish death stares dodgy dealings shady agreements never far away would say main appeal show due fact goes shows would dare forget pretty pictures painted mainstream audiences forget charm forget romance oz mess around first episode ever saw struck nasty surreal could say ready watched developed taste oz got accustomed high levels graphic violence violence injustice crooked guards sold nickel inmates kill order get away well mannered middle class inmates turned prison bitches due lack street skills prison experience watching oz may become comfortable uncomfortable viewing get touch darker side',\n 'wonderful little production filming technique unassuming old time bbc fashion gives comforting sometimes discomforting sense realism entire piece actors extremely well chosen michael sheen got voices pat truly see seamless editing guided references williams diary entries well worth watching terrificly written performed piece masterful production one great master comedy life realism really comes home little things fantasy guard rather use traditional dream techniques remains solid disappears plays knowledge senses particularly scenes concerning orton halliwell sets particularly flat halliwell decorating every surface terribly well done']"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df['clean_reviews'], df['sentiment'], random_state=42, test_size=0.2, stratify=df['sentiment'])\nX_test_ids = list(X_test.index)\nX_test_ids[:5]","metadata":{"execution":{"iopub.status.busy":"2022-12-24T04:11:11.989041Z","iopub.execute_input":"2022-12-24T04:11:11.989495Z","iopub.status.idle":"2022-12-24T04:11:12.059320Z","shell.execute_reply.started":"2022-12-24T04:11:11.989454Z","shell.execute_reply":"2022-12-24T04:11:12.058341Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"[18870, 39791, 30381, 42294, 33480]"},"metadata":{}}]},{"cell_type":"markdown","source":"## Prepare data","metadata":{}},{"cell_type":"code","source":"dataset_name = 'imdb'\nsentences = list(df['clean_reviews'])\nlabels = list(df['sentiment'])\ntrain_or_test_list = ['test' if idx in X_test_ids else 'train' for idx in df.index]\n\nmeta_data_list = []\n\nfor i in range(len(sentences)):\n    meta = str(i) + '\\t' + train_or_test_list[i] + '\\t' + labels[i]\n    meta_data_list.append(meta)\n\nmeta_data_str = '\\n'.join(meta_data_list)\n\nf = open(dataset_name + '.txt', 'w')\nf.write(meta_data_str)\nf.close()\n\ncorpus_str = '\\n'.join(sentences)\n\nf = open(dataset_name + '.corpus.txt', 'w')\nf.write(corpus_str)\nf.close()","metadata":{"execution":{"iopub.status.busy":"2022-12-23T15:28:32.791735Z","iopub.execute_input":"2022-12-23T15:28:32.792183Z","iopub.status.idle":"2022-12-23T15:28:41.807907Z","shell.execute_reply.started":"2022-12-23T15:28:32.792140Z","shell.execute_reply":"2022-12-23T15:28:41.806891Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"markdown","source":"Jangan lupa ganti path ke dataset","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pickle as pkl\nimport networkx as nx\nimport scipy.sparse as sp\nfrom scipy.sparse.linalg.eigen.arpack import eigsh\nimport sys\nimport re\n\n\ndef parse_index_file(filename):\n    \"\"\"Parse index file.\"\"\"\n    index = []\n    for line in open(filename):\n        index.append(int(line.strip()))\n    return index\n\n\ndef sample_mask(idx, l):\n    \"\"\"Create mask.\"\"\"\n    mask = np.zeros(l)\n    mask[idx] = 1\n    return np.array(mask, dtype=np.bool)\n\n\ndef load_data(dataset_str):\n    \"\"\"\n    Loads input data from gcn/data directory\n    ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n    ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n    ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n    ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n    ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n        object;\n    ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n    All objects above must be saved using python pickle module.\n    :param dataset_str: Dataset name\n    :return: All data input files loaded (as well the training/test data).\n    \"\"\"\n    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'adj']\n    objects = []\n    for i in range(len(names)):\n        with open(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews-clean/data/data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n            if sys.version_info > (3, 0):\n                objects.append(pkl.load(f, encoding='latin1'))\n            else:\n                objects.append(pkl.load(f))\n\n    x, y, tx, ty, allx, ally, graph = tuple(objects)\n    test_idx_reorder = parse_index_file(\n        \"/kaggle/input/imdb-dataset-of-50k-movie-reviews-clean/data/data/{}.test.index\".format(dataset_str))\n    test_idx_range = np.sort(test_idx_reorder)\n    print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n\n    # training nodes are training docs, no initial features\n    # print(\"x: \", x)\n    # test nodes are training docs, no initial features\n    # print(\"tx: \", tx)\n    # both labeled and unlabeled training instances are training docs and words\n    # print(\"allx: \", allx)\n    # training labels are training doc labels\n    # print(\"y: \", y)\n    # test labels are test doc labels\n    # print(\"ty: \", ty)\n    # ally are labels for labels for allx, some will not have labels, i.e., all 0\n    # print(\"ally: \\n\")\n    # for i in ally:\n    # if(sum(i) == 0):\n    # print(i)\n    # graph edge weight is the word co-occurence or doc word frequency\n    # no need to build map, directly build csr_matrix\n    # print('graph : ', graph)\n\n    if dataset_str == 'citeseer':\n        # Fix citeseer dataset (there are some isolated nodes in the graph)\n        # Find isolated nodes, add them as zero-vecs into the right position\n        test_idx_range_full = range(\n            min(test_idx_reorder), max(test_idx_reorder)+1)\n        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n        tx = tx_extended\n        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n        ty = ty_extended\n\n    features = sp.vstack((allx, tx)).tolil()\n    features[test_idx_reorder, :] = features[test_idx_range, :]\n    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n\n    labels = np.vstack((ally, ty))\n    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n    # print(len(labels))\n\n    idx_test = test_idx_range.tolist()\n    # print(idx_test)\n    idx_train = range(len(y))\n    idx_val = range(len(y), len(y)+500)\n\n    train_mask = sample_mask(idx_train, labels.shape[0])\n    val_mask = sample_mask(idx_val, labels.shape[0])\n    test_mask = sample_mask(idx_test, labels.shape[0])\n\n    y_train = np.zeros(labels.shape)\n    y_val = np.zeros(labels.shape)\n    y_test = np.zeros(labels.shape)\n    y_train[train_mask, :] = labels[train_mask, :]\n    y_val[val_mask, :] = labels[val_mask, :]\n    y_test[test_mask, :] = labels[test_mask, :]\n\n    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask\n\n\ndef load_corpus(dataset_str):\n    \"\"\"\n    Loads input corpus from gcn/data directory\n    ind.dataset_str.x => the feature vectors of the training docs as scipy.sparse.csr.csr_matrix object;\n    ind.dataset_str.tx => the feature vectors of the test docs as scipy.sparse.csr.csr_matrix object;\n    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training docs/words\n        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n    ind.dataset_str.y => the one-hot labels of the labeled training docs as numpy.ndarray object;\n    ind.dataset_str.ty => the one-hot labels of the test docs as numpy.ndarray object;\n    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n    ind.dataset_str.adj => adjacency matrix of word/doc nodes as scipy.sparse.csr.csr_matrix object;\n    ind.dataset_str.train.index => the indices of training docs in original doc list.\n    All objects above must be saved using python pickle module.\n    :param dataset_str: Dataset name\n    :return: All data input files loaded (as well the training/test data).\n    \"\"\"\n\n    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'adj']\n    objects = []\n    for i in range(len(names)):\n#         print(f\"/kaggle/input/imdb-dataset-of-50k-movie-reviews-clean/data/data/ind.{dataset_str}.{names[i]}\")\n        \n        with open(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews-clean/data/data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n            if sys.version_info > (3, 0):\n                objects.append(pkl.load(f, encoding='latin1'))\n            else:\n                objects.append(pkl.load(f))\n\n    x, y, tx, ty, allx, ally, adj = tuple(objects)\n    print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n\n    features = sp.vstack((allx, tx)).tolil()\n    labels = np.vstack((ally, ty))\n    print(len(labels))\n\n    train_idx_orig = parse_index_file(\n        \"/kaggle/input/imdb-dataset-of-50k-movie-reviews-clean/data/data/{}.train.index\".format(dataset_str))\n    train_size = len(train_idx_orig)\n\n    val_size = train_size - x.shape[0]\n    test_size = tx.shape[0]\n\n    idx_train = range(len(y))\n    idx_val = range(len(y), len(y) + val_size)\n    idx_test = range(allx.shape[0], allx.shape[0] + test_size)\n\n    train_mask = sample_mask(idx_train, labels.shape[0])\n    val_mask = sample_mask(idx_val, labels.shape[0])\n    test_mask = sample_mask(idx_test, labels.shape[0])\n\n    y_train = np.zeros(labels.shape)\n    y_val = np.zeros(labels.shape)\n    y_test = np.zeros(labels.shape)\n    y_train[train_mask, :] = labels[train_mask, :]\n    y_val[val_mask, :] = labels[val_mask, :]\n    y_test[test_mask, :] = labels[test_mask, :]\n\n    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n\n    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size\n\n\ndef sparse_to_tuple(sparse_mx):\n    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n    def to_tuple(mx):\n        if not sp.isspmatrix_coo(mx):\n            mx = mx.tocoo()\n        coords = np.vstack((mx.row, mx.col)).transpose()\n        values = mx.data\n        shape = mx.shape\n        return coords, values, shape\n\n    if isinstance(sparse_mx, list):\n        for i in range(len(sparse_mx)):\n            sparse_mx[i] = to_tuple(sparse_mx[i])\n    else:\n        sparse_mx = to_tuple(sparse_mx)\n\n    return sparse_mx\n\n\ndef preprocess_features(features):\n    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n    rowsum = np.array(features.sum(1))\n    r_inv = np.power(rowsum, -1).flatten()\n    r_inv[np.isinf(r_inv)] = 0.\n    r_mat_inv = sp.diags(r_inv)\n    features = r_mat_inv.dot(features)\n    return sparse_to_tuple(features)\n\n\ndef normalize_adj(adj):\n    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n    adj = sp.coo_matrix(adj)\n    rowsum = np.array(adj.sum(1))\n    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n\n\ndef preprocess_adj(adj):\n    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n    return sparse_to_tuple(adj_normalized)\n\n\ndef construct_feed_dict(features, support, labels, labels_mask, placeholders):\n    \"\"\"Construct feed dictionary.\"\"\"\n    feed_dict = dict()\n    feed_dict.update({placeholders['labels']: labels})\n    feed_dict.update({placeholders['labels_mask']: labels_mask})\n    feed_dict.update({placeholders['features']: features})\n    feed_dict.update({placeholders['support'][i]: support[i]\n                      for i in range(len(support))})\n    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n    return feed_dict\n\n\ndef chebyshev_polynomials(adj, k):\n    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\"\"\"\n    print(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n\n    adj_normalized = normalize_adj(adj)\n    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n    largest_eigval, _ = eigsh(laplacian, 1, which='LM')\n    scaled_laplacian = (\n        2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n\n    t_k = list()\n    t_k.append(sp.eye(adj.shape[0]))\n    t_k.append(scaled_laplacian)\n\n    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n\n    for i in range(2, k+1):\n        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n\n    return sparse_to_tuple(t_k)\n\n\ndef loadWord2Vec(filename):\n    \"\"\"Read Word Vectors\"\"\"\n    vocab = []\n    embd = []\n    word_vector_map = {}\n    file = open(filename, 'r')\n    for line in file.readlines():\n        row = line.strip().split(' ')\n        if(len(row) > 2):\n            vocab.append(row[0])\n            vector = row[1:]\n            length = len(vector)\n            for i in range(length):\n                vector[i] = float(vector[i])\n            embd.append(vector)\n            word_vector_map[row[0]] = vector\n    print('Loaded Word Vectors!')\n    file.close()\n    return vocab, embd, word_vector_map\n\ndef clean_str(string):\n    \"\"\"\n    Tokenization/string cleaning for all datasets except for SST.\n    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n    \"\"\"\n    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n    string = re.sub(r\"\\'s\", \" \\'s\", string)\n    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n    string = re.sub(r\"\\'re\", \" \\'re\", string)\n    string = re.sub(r\"\\'d\", \" \\'d\", string)\n    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n    string = re.sub(r\",\", \" , \", string)\n    string = re.sub(r\"!\", \" ! \", string)\n    string = re.sub(r\"\\(\", \" \\( \", string)\n    string = re.sub(r\"\\)\", \" \\) \", string)\n    string = re.sub(r\"\\?\", \" \\? \", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n    return string.strip().lower()","metadata":{"execution":{"iopub.status.busy":"2023-01-18T15:14:31.748944Z","iopub.execute_input":"2023-01-18T15:14:31.749373Z","iopub.status.idle":"2023-01-18T15:14:32.240194Z","shell.execute_reply.started":"2023-01-18T15:14:31.749287Z","shell.execute_reply":"2023-01-18T15:14:32.239295Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Remove words","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nimport nltk\nfrom nltk.wsd import lesk\nfrom nltk.corpus import wordnet as wn\nimport sys\n\ndataset = 'imdb'\n\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\nprint(stop_words)\n\n# Read Word Vectors\n# word_vector_file = 'data/glove.6B/glove.6B.200d.txt'\n# vocab, embd, word_vector_map = loadWord2Vec(word_vector_file)\n# word_embeddings_dim = len(embd[0])\n# dataset = '20ng'\n\ndoc_content_list = []\nf = open(dataset + '.corpus.txt', 'rb')\n# f = open('data/wiki_long_abstracts_en_text.txt', 'r')\nfor line in f.readlines():\n    doc_content_list.append(line.strip().decode('latin1'))\nf.close()\n\n\nword_freq = {}  # to remove rare words\n\nfor doc_content in doc_content_list:\n    temp = clean_str(doc_content)\n    words = temp.split()\n    for word in words:\n        if word in word_freq:\n            word_freq[word] += 1\n        else:\n            word_freq[word] = 1\n\nclean_docs = []\nfor doc_content in doc_content_list:\n    temp = clean_str(doc_content)\n    words = temp.split()\n    doc_words = []\n    for word in words:\n        # word not in stop_words and word_freq[word] >= 5\n        if dataset == 'mr':\n            doc_words.append(word)\n        elif word not in stop_words and word_freq[word] >= 5:\n            doc_words.append(word)\n\n    doc_str = ' '.join(doc_words).strip()\n    #if doc_str == '':\n        #doc_str = temp\n    clean_docs.append(doc_str)\n\nclean_corpus_str = '\\n'.join(clean_docs)\n\nf = open(dataset + '.corpus.clean.txt', 'w')\n#f = open('data/wiki_long_abstracts_en_text.clean.txt', 'w')\nf.write(clean_corpus_str)\nf.close()\n\n#dataset = '20ng'\nmin_len = 10000\naver_len = 0\nmax_len = 0 \n\nf = open(dataset + '.corpus.clean.txt', 'r')\n#f = open('data/wiki_long_abstracts_en_text.txt', 'r')\nlines = f.readlines()\nfor line in lines:\n    line = line.strip()\n    temp = line.split()\n    aver_len = aver_len + len(temp)\n    if len(temp) < min_len:\n        min_len = len(temp)\n    if len(temp) > max_len:\n        max_len = len(temp)\nf.close()\naver_len = 1.0 * aver_len / len(lines)\nprint('min_len : ' + str(min_len))\nprint('max_len : ' + str(max_len))\nprint('average_len : ' + str(aver_len))","metadata":{"execution":{"iopub.status.busy":"2022-12-23T15:42:27.865404Z","iopub.execute_input":"2022-12-23T15:42:27.865874Z","iopub.status.idle":"2022-12-23T15:42:42.258420Z","shell.execute_reply.started":"2022-12-23T15:42:27.865836Z","shell.execute_reply":"2022-12-23T15:42:42.257251Z"},"jupyter":{"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"{'shouldn', 'shan', 'his', 'being', 'on', 'their', 'so', 'who', 'those', 'no', 'the', 'isn', 'above', 'weren', \"hadn't\", 'over', 'because', 'which', \"didn't\", 'yourself', 'needn', 'most', 'any', \"needn't\", 'doing', 'i', 'through', \"mustn't\", 'them', 'before', 'each', 'are', 'wasn', \"hasn't\", 'will', 'and', 'mustn', 'only', 'yourselves', 'does', 'own', 'now', 'just', 'if', 't', 'yours', 'me', 'at', 'not', 'a', 'both', 'aren', \"aren't\", 'was', 'ma', 'few', 'out', 'am', 'ain', 've', \"doesn't\", 'couldn', 'again', \"mightn't\", \"shan't\", 'my', 'here', 'as', 'should', 'there', \"wouldn't\", 'too', 'between', \"that'll\", 'all', 'that', \"shouldn't\", 'into', 'whom', 'this', 'against', 'why', 'herself', 'didn', 'haven', 'of', 'than', 'other', 'is', 'off', 'an', 'below', \"it's\", \"you'll\", \"won't\", 'for', 'him', 'it', 'll', 'hers', 'by', 'but', 'once', 'some', 'your', 'can', 'our', 'won', 'y', 'or', 'have', 'did', 'he', 'in', 'don', 'hadn', \"you're\", 'themselves', 'such', 'had', \"isn't\", 'itself', 'her', 'what', 'while', 'its', 'they', 'after', \"couldn't\", 'when', 'nor', 'doesn', 'same', 'ours', 'very', 'hasn', \"should've\", \"haven't\", 'during', 'about', 'from', 'were', 'more', 'you', 'theirs', 'wouldn', 'we', 'having', \"you've\", 'until', 'then', \"don't\", 'd', 'o', 'how', 'these', 'ourselves', 'down', \"weren't\", \"wasn't\", 'do', 'with', 'myself', 'himself', 'been', 'to', 'be', \"you'd\", \"she's\", 'has', 'under', 'she', 'm', 'mightn', 'up', 's', 'further', 'where', 're'}\nmin_len : 3\nmax_len : 1387\naverage_len : 117.88286\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Build Graph","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pickle as pkl\nimport networkx as nx\nimport scipy.sparse as sp\nfrom math import log\nfrom sklearn import svm\nfrom nltk.corpus import wordnet as wn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport sys\nfrom scipy.spatial.distance import cosine\nnltk.download(['omw-1.4', 'wordnet'])\n\n# build corpus\ndataset = 'imdb'\n\n# Read Word Vectors\n# word_vector_file = 'data/glove.6B/glove.6B.300d.txt'\n# word_vector_file = 'data/corpus/' + dataset + '_word_vectors.txt'\n#_, embd, word_vector_map = loadWord2Vec(word_vector_file)\n# word_embeddings_dim = len(embd[0])\n\nword_embeddings_dim = 300\nword_vector_map = {}\n\n# shulffing\ndoc_name_list = []\ndoc_train_list = []\ndoc_test_list = []\n\nf = open(dataset + '.txt', 'r')\nlines = f.readlines()\nfor line in lines:\n    doc_name_list.append(line.strip())\n    temp = line.split(\"\\t\")\n    if temp[1].find('test') != -1:\n        doc_test_list.append(line.strip())\n    elif temp[1].find('train') != -1:\n        doc_train_list.append(line.strip())\nf.close()\n# print(doc_train_list)\n# print(doc_test_list)\n\ndoc_content_list = []\nf = open(dataset + '.corpus.clean.txt', 'r')\nlines = f.readlines()\nfor line in lines:\n    doc_content_list.append(line.strip())\nf.close()\n# print(doc_content_list)\n\ntrain_ids = []\nfor train_name in doc_train_list:\n    train_id = doc_name_list.index(train_name)\n    train_ids.append(train_id)\nprint(train_ids[:5])\nrandom.shuffle(train_ids)\n\n# partial labeled data\n#train_ids = train_ids[:int(0.2 * len(train_ids))]\n\ntrain_ids_str = '\\n'.join(str(index) for index in train_ids)\nf = open(dataset + '.train.index', 'w')\nf.write(train_ids_str)\nf.close()\n\ntest_ids = []\nfor test_name in doc_test_list:\n    test_id = doc_name_list.index(test_name)\n    test_ids.append(test_id)\nprint(test_ids[:5])\nrandom.shuffle(test_ids)\n\ntest_ids_str = '\\n'.join(str(index) for index in test_ids)\nf = open(dataset + '.test.index', 'w')\nf.write(test_ids_str)\nf.close()\n\nids = train_ids + test_ids\nprint(len(ids))\n\nshuffle_doc_name_list = []\nshuffle_doc_words_list = []\nfor id in ids:\n    shuffle_doc_name_list.append(doc_name_list[int(id)])\n    shuffle_doc_words_list.append(doc_content_list[int(id)])\nshuffle_doc_name_str = '\\n'.join(shuffle_doc_name_list)\nshuffle_doc_words_str = '\\n'.join(shuffle_doc_words_list)\n\nf = open(dataset + '_shuffle.txt', 'w')\nf.write(shuffle_doc_name_str)\nf.close()\n\nf = open(dataset + '_shuffle.corpus.txt', 'w')\nf.write(shuffle_doc_words_str)\nf.close()\n\n# build vocab\nword_freq = {}\nword_set = set()\nfor doc_words in shuffle_doc_words_list:\n    words = doc_words.split()\n    for word in words:\n        word_set.add(word)\n        if word in word_freq:\n            word_freq[word] += 1\n        else:\n            word_freq[word] = 1\n\nvocab = list(word_set)\nvocab_size = len(vocab)\n\nword_doc_list = {}\n\nfor i in range(len(shuffle_doc_words_list)):\n    doc_words = shuffle_doc_words_list[i]\n    words = doc_words.split()\n    appeared = set()\n    for word in words:\n        if word in appeared:\n            continue\n        if word in word_doc_list:\n            doc_list = word_doc_list[word]\n            doc_list.append(i)\n            word_doc_list[word] = doc_list\n        else:\n            word_doc_list[word] = [i]\n        appeared.add(word)\n\nword_doc_freq = {}\nfor word, doc_list in word_doc_list.items():\n    word_doc_freq[word] = len(doc_list)\n\nword_id_map = {}\nfor i in range(vocab_size):\n    word_id_map[vocab[i]] = i\n\nvocab_str = '\\n'.join(vocab)\n\nf = open(dataset + '_vocab.corpus.txt', 'w')\nf.write(vocab_str)\nf.close()\n\n'''\nWord definitions begin\n'''\ndefinitions = []\nfor word in vocab:\n    word = word.strip()\n    synsets = wn.synsets(clean_str(word))\n    word_defs = []\n    for synset in synsets:\n        syn_def = synset.definition()\n        word_defs.append(syn_def)\n    word_des = ' '.join(word_defs)\n    if word_des == '':\n        word_des = '<PAD>'\n    definitions.append(word_des)\nstring = '\\n'.join(definitions)\nf = open(dataset + '_vocab_def.corpus.txt', 'w')\nf.write(string)\nf.close()\ntfidf_vec = TfidfVectorizer(max_features=1000)\ntfidf_matrix = tfidf_vec.fit_transform(definitions)\ntfidf_matrix_array = tfidf_matrix.toarray()\nprint(tfidf_matrix_array[0], len(tfidf_matrix_array[0]))\nword_vectors = []\nfor i in range(len(vocab)):\n    word = vocab[i]\n    vector = tfidf_matrix_array[i]\n    str_vector = []\n    for j in range(len(vector)):\n        str_vector.append(str(vector[j]))\n    temp = ' '.join(str_vector)\n    word_vector = word + ' ' + temp\n    word_vectors.append(word_vector)\nstring = '\\n'.join(word_vectors)\nf = open(dataset + '_word_vectors.corpus.txt', 'w')\nf.write(string)\nf.close()\nword_vector_file = dataset + '_word_vectors.corpus.txt'\n_, embd, word_vector_map = loadWord2Vec(word_vector_file)\nword_embeddings_dim = len(embd[0])\n\n'''\nWord definitions end\n'''\n\n# label list\nlabel_set = set()\nfor doc_meta in shuffle_doc_name_list:\n    temp = doc_meta.split('\\t')\n    label_set.add(temp[2])\nlabel_list = list(label_set)\n\nlabel_list_str = '\\n'.join(label_list)\nf = open(dataset + '_labels.corpus.txt', 'w')\nf.write(label_list_str)\nf.close()\n\n# x: feature vectors of training docs, no initial features\n# slect 90% training set\ntrain_size = len(train_ids)\nval_size = int(0.1 * train_size)\nreal_train_size = train_size - val_size  # - int(0.5 * train_size)\n# different training rates\n\nreal_train_doc_names = shuffle_doc_name_list[:real_train_size]\nreal_train_doc_names_str = '\\n'.join(real_train_doc_names)\n\nf = open(dataset + '.real_train.name', 'w')\nf.write(real_train_doc_names_str)\nf.close()\n\nrow_x = []\ncol_x = []\ndata_x = []\nfor i in range(real_train_size):\n    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n    doc_words = shuffle_doc_words_list[i]\n    words = doc_words.split()\n    doc_len = len(words)\n    for word in words:\n        if word in word_vector_map:\n            word_vector = word_vector_map[word]\n            # print(doc_vec)\n            # print(np.array(word_vector))\n            doc_vec = doc_vec + np.array(word_vector)\n\n    for j in range(word_embeddings_dim):\n        row_x.append(i)\n        col_x.append(j)\n        # np.random.uniform(-0.25, 0.25)\n        data_x.append(doc_vec[j] / doc_len)  # doc_vec[j]/ doc_len\n\n# x = sp.csr_matrix((real_train_size, word_embeddings_dim), dtype=np.float32)\nx = sp.csr_matrix((data_x, (row_x, col_x)), shape=(\n    real_train_size, word_embeddings_dim))\n\ny = []\nfor i in range(real_train_size):\n    doc_meta = shuffle_doc_name_list[i]\n    temp = doc_meta.split('\\t')\n    label = temp[2]\n    one_hot = [0 for l in range(len(label_list))]\n    label_index = label_list.index(label)\n    one_hot[label_index] = 1\n    y.append(one_hot)\ny = np.array(y)\n\n# tx: feature vectors of test docs, no initial features\ntest_size = len(test_ids)\n\nrow_tx = []\ncol_tx = []\ndata_tx = []\nfor i in range(test_size):\n    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n    doc_words = shuffle_doc_words_list[i + train_size]\n    words = doc_words.split()\n    doc_len = len(words)\n    for word in words:\n        if word in word_vector_map:\n            word_vector = word_vector_map[word]\n            doc_vec = doc_vec + np.array(word_vector)\n\n    for j in range(word_embeddings_dim):\n        row_tx.append(i)\n        col_tx.append(j)\n        # np.random.uniform(-0.25, 0.25)\n        data_tx.append(doc_vec[j] / doc_len)  # doc_vec[j] / doc_len\n\n# tx = sp.csr_matrix((test_size, word_embeddings_dim), dtype=np.float32)\ntx = sp.csr_matrix((data_tx, (row_tx, col_tx)),\n                   shape=(test_size, word_embeddings_dim))\n\nty = []\nfor i in range(test_size):\n    doc_meta = shuffle_doc_name_list[i + train_size]\n    temp = doc_meta.split('\\t')\n    label = temp[2]\n    one_hot = [0 for l in range(len(label_list))]\n    label_index = label_list.index(label)\n    one_hot[label_index] = 1\n    ty.append(one_hot)\nty = np.array(ty)\n\n# allx: the the feature vectors of both labeled and unlabeled training instances\n# (a superset of x)\n# unlabeled training instances -> words\n\nword_vectors = np.random.uniform(-0.01, 0.01,\n                                 (vocab_size, word_embeddings_dim))\n\nfor i in range(len(vocab)):\n    word = vocab[i]\n    if word in word_vector_map:\n        vector = word_vector_map[word]\n        word_vectors[i] = vector\n\nrow_allx = []\ncol_allx = []\ndata_allx = []\n\nfor i in range(train_size):\n    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n    doc_words = shuffle_doc_words_list[i]\n    words = doc_words.split()\n    doc_len = len(words)\n    for word in words:\n        if word in word_vector_map:\n            word_vector = word_vector_map[word]\n            doc_vec = doc_vec + np.array(word_vector)\n\n    for j in range(word_embeddings_dim):\n        row_allx.append(int(i))\n        col_allx.append(j)\n        # np.random.uniform(-0.25, 0.25)\n        data_allx.append(doc_vec[j] / doc_len)  # doc_vec[j]/doc_len\nfor i in range(vocab_size):\n    for j in range(word_embeddings_dim):\n        row_allx.append(int(i + train_size))\n        col_allx.append(j)\n        data_allx.append(word_vectors.item((i, j)))\n\n\nrow_allx = np.array(row_allx)\ncol_allx = np.array(col_allx)\ndata_allx = np.array(data_allx)\n\nallx = sp.csr_matrix(\n    (data_allx, (row_allx, col_allx)), shape=(train_size + vocab_size, word_embeddings_dim))\n\nally = []\nfor i in range(train_size):\n    doc_meta = shuffle_doc_name_list[i]\n    temp = doc_meta.split('\\t')\n    label = temp[2]\n    one_hot = [0 for l in range(len(label_list))]\n    label_index = label_list.index(label)\n    one_hot[label_index] = 1\n    ally.append(one_hot)\n\nfor i in range(vocab_size):\n    one_hot = [0 for l in range(len(label_list))]\n    ally.append(one_hot)\n\nally = np.array(ally)\n\nprint(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n\n'''\nDoc word heterogeneous graph\n'''\n\n# word co-occurence with context windows\nwindow_size = 20\nwindows = []\n\nfor doc_words in shuffle_doc_words_list:\n    words = doc_words.split()\n    length = len(words)\n    if length <= window_size:\n        windows.append(words)\n    else:\n        # print(length, length - window_size + 1)\n        for j in range(length - window_size + 1):\n            window = words[j: j + window_size]\n            windows.append(window)\n            # print(window)\n\n\nword_window_freq = {}\nfor window in windows:\n    appeared = set()\n    for i in range(len(window)):\n        if window[i] in appeared:\n            continue\n        if window[i] in word_window_freq:\n            word_window_freq[window[i]] += 1\n        else:\n            word_window_freq[window[i]] = 1\n        appeared.add(window[i])\n\nword_pair_count = {}\nfor window in windows:\n    for i in range(1, len(window)):\n        for j in range(0, i):\n            word_i = window[i]\n            word_i_id = word_id_map[word_i]\n            word_j = window[j]\n            word_j_id = word_id_map[word_j]\n            if word_i_id == word_j_id:\n                continue\n            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n            if word_pair_str in word_pair_count:\n                word_pair_count[word_pair_str] += 1\n            else:\n                word_pair_count[word_pair_str] = 1\n            # two orders\n            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n            if word_pair_str in word_pair_count:\n                word_pair_count[word_pair_str] += 1\n            else:\n                word_pair_count[word_pair_str] = 1\n\nrow = []\ncol = []\nweight = []\n\n# pmi as weights\n\nnum_window = len(windows)\n\nfor key in word_pair_count:\n    temp = key.split(',')\n    i = int(temp[0])\n    j = int(temp[1])\n    count = word_pair_count[key]\n    word_freq_i = word_window_freq[vocab[i]]\n    word_freq_j = word_window_freq[vocab[j]]\n    pmi = log((1.0 * count / num_window) /\n              (1.0 * word_freq_i * word_freq_j/(num_window * num_window)))\n    if pmi <= 0:\n        continue\n    row.append(train_size + i)\n    col.append(train_size + j)\n    weight.append(pmi)\n\n# word vector cosine similarity as weights\n\n'''\nfor i in range(vocab_size):\n    for j in range(vocab_size):\n        if vocab[i] in word_vector_map and vocab[j] in word_vector_map:\n            vector_i = np.array(word_vector_map[vocab[i]])\n            vector_j = np.array(word_vector_map[vocab[j]])\n            similarity = 1.0 - cosine(vector_i, vector_j)\n            if similarity > 0.9:\n                print(vocab[i], vocab[j], similarity)\n                row.append(train_size + i)\n                col.append(train_size + j)\n                weight.append(similarity)\n'''\n# doc word frequency\ndoc_word_freq = {}\n\nfor doc_id in range(len(shuffle_doc_words_list)):\n    doc_words = shuffle_doc_words_list[doc_id]\n    words = doc_words.split()\n    for word in words:\n        word_id = word_id_map[word]\n        doc_word_str = str(doc_id) + ',' + str(word_id)\n        if doc_word_str in doc_word_freq:\n            doc_word_freq[doc_word_str] += 1\n        else:\n            doc_word_freq[doc_word_str] = 1\n\nfor i in range(len(shuffle_doc_words_list)):\n    doc_words = shuffle_doc_words_list[i]\n    words = doc_words.split()\n    doc_word_set = set()\n    for word in words:\n        if word in doc_word_set:\n            continue\n        j = word_id_map[word]\n        key = str(i) + ',' + str(j)\n        freq = doc_word_freq[key]\n        if i < train_size:\n            row.append(i)\n        else:\n            row.append(i + vocab_size)\n        col.append(train_size + j)\n        idf = log(1.0 * len(shuffle_doc_words_list) /\n                  word_doc_freq[vocab[j]])\n        weight.append(freq * idf)\n        doc_word_set.add(word)\n\nnode_size = train_size + vocab_size + test_size\nadj = sp.csr_matrix(\n    (weight, (row, col)), shape=(node_size, node_size))\n\n# dump objects\nf = open(\"ind.{}.x\".format(dataset), 'wb')\npkl.dump(x, f)\nf.close()\n\nf = open(\"ind.{}.y\".format(dataset), 'wb')\npkl.dump(y, f)\nf.close()\n\nf = open(\"ind.{}.tx\".format(dataset), 'wb')\npkl.dump(tx, f)\nf.close()\n\nf = open(\"ind.{}.ty\".format(dataset), 'wb')\npkl.dump(ty, f)\nf.close()\n\nf = open(\"ind.{}.allx\".format(dataset), 'wb')\npkl.dump(allx, f)\nf.close()\n\nf = open(\"ind.{}.ally\".format(dataset), 'wb')\npkl.dump(ally, f)\nf.close()\n\nf = open(\"ind.{}.adj\".format(dataset), 'wb')\npkl.dump(adj, f)\nf.close()","metadata":{"execution":{"iopub.status.busy":"2022-12-23T15:53:52.432081Z","iopub.execute_input":"2022-12-23T15:53:52.432512Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"[0, 1, 3, 5, 6]\n[2, 4, 16, 19, 23]\n50000\n[0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.40734651\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.40288035 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.41472958 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.39188297 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.39081469 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.10879489\n 0.         0.         0.         0.         0.         0.\n 0.         0.20395753 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.12032707\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.12998809 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.16736781 0.         0.28394496 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.        ] 1000\nLoaded Word Vectors!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Inits","metadata":{}},{"cell_type":"code","source":"import tensorflow.compat.v1 as tf\nimport numpy as np\n\n\ndef uniform(shape, scale=0.05, name=None):\n    \"\"\"Uniform init.\"\"\"\n    initial = tf.random_uniform(shape, minval=-scale, maxval=scale, dtype=tf.float32)\n    return tf.Variable(initial, name=name)\n\n\ndef glorot(shape, name=None):\n    \"\"\"Glorot & Bengio (AISTATS 2010) init.\"\"\"\n    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n    initial = tf.random_uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32)\n    return tf.Variable(initial, name=name)\n\n\ndef zeros(shape, name=None):\n    \"\"\"All zeros.\"\"\"\n    initial = tf.zeros(shape, dtype=tf.float32)\n    return tf.Variable(initial, name=name)\n\n\ndef ones(shape, name=None):\n    \"\"\"All ones.\"\"\"\n    initial = tf.ones(shape, dtype=tf.float32)\n    return tf.Variable(initial, name=name)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T15:14:47.826454Z","iopub.execute_input":"2023-01-18T15:14:47.826829Z","iopub.status.idle":"2023-01-18T15:14:52.055264Z","shell.execute_reply.started":"2023-01-18T15:14:47.826798Z","shell.execute_reply":"2023-01-18T15:14:52.054308Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Metrics","metadata":{}},{"cell_type":"code","source":"def masked_softmax_cross_entropy(preds, labels, mask):\n    \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n    print(preds)\n    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)\n    mask = tf.cast(mask, dtype=tf.float32)\n    mask /= tf.reduce_mean(mask)\n    loss *= mask\n    return tf.reduce_mean(loss)\n\n\ndef masked_accuracy(preds, labels, mask):\n    \"\"\"Accuracy with masking.\"\"\"\n    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n\n    accuracy_all = tf.cast(correct_prediction, tf.float32)\n    mask = tf.cast(mask, dtype=tf.float32)\n    mask /= tf.reduce_mean(mask)\n    accuracy_all *= mask\n    return tf.reduce_mean(accuracy_all)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T15:14:52.066767Z","iopub.execute_input":"2023-01-18T15:14:52.068495Z","iopub.status.idle":"2023-01-18T15:14:52.079038Z","shell.execute_reply.started":"2023-01-18T15:14:52.068447Z","shell.execute_reply":"2023-01-18T15:14:52.078062Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Layers","metadata":{}},{"cell_type":"code","source":"flags = tf.app.flags\nFLAGS = flags.FLAGS\n\n# global unique layer ID dictionary for layer name assignment\n_LAYER_UIDS = {}\n\n\ndef get_layer_uid(layer_name=''):\n    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n    if layer_name not in _LAYER_UIDS:\n        _LAYER_UIDS[layer_name] = 1\n        return 1\n    else:\n        _LAYER_UIDS[layer_name] += 1\n        return _LAYER_UIDS[layer_name]\n\n\ndef sparse_dropout(x, keep_prob, noise_shape):\n    \"\"\"Dropout for sparse tensors.\"\"\"\n    random_tensor = keep_prob\n    random_tensor += tf.random_uniform(noise_shape)\n    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n    pre_out = tf.sparse_retain(x, dropout_mask)\n    return pre_out * (1./keep_prob)\n\n\ndef dot(x, y, sparse=False):\n    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n    if sparse:\n        res = tf.sparse_tensor_dense_matmul(x, y)\n    else:\n        res = tf.matmul(x, y)\n    return res\n\n\nclass Layer(object):\n    \"\"\"Base layer class. Defines basic API for all layer objects.\n    Implementation inspired by keras (http://keras.io).\n    # Properties\n        name: String, defines the variable scope of the layer.\n        logging: Boolean, switches Tensorflow histogram logging on/off\n    # Methods\n        _call(inputs): Defines computation graph of layer\n            (i.e. takes input, returns output)\n        __call__(inputs): Wrapper for _call()\n        _log_vars(): Log all variables\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        allowed_kwargs = {'name', 'logging'}\n        for kwarg in kwargs.keys():\n            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n        name = kwargs.get('name')\n        if not name:\n            layer = self.__class__.__name__.lower()\n            name = layer + '_' + str(get_layer_uid(layer))\n        self.name = name\n        self.vars = {}\n        logging = kwargs.get('logging', False)\n        self.logging = logging\n        self.sparse_inputs = False\n\n    def _call(self, inputs):\n        return inputs\n\n    def __call__(self, inputs):\n        with tf.name_scope(self.name):\n            if self.logging and not self.sparse_inputs:\n                tf.summary.histogram(self.name + '/inputs', inputs)\n            outputs = self._call(inputs)\n            if self.logging:\n                tf.summary.histogram(self.name + '/outputs', outputs)\n            return outputs\n\n    def _log_vars(self):\n        for var in self.vars:\n            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n\n\nclass Dense(Layer):\n    \"\"\"Dense layer.\"\"\"\n    def __init__(self, input_dim, output_dim, placeholders, dropout=0., sparse_inputs=False,\n                 act=tf.nn.relu, bias=False, featureless=False, **kwargs):\n        super(Dense, self).__init__(**kwargs)\n\n        if dropout:\n            self.dropout = placeholders['dropout']\n        else:\n            self.dropout = 0.\n\n        self.act = act\n        self.sparse_inputs = sparse_inputs\n        self.featureless = featureless\n        self.bias = bias\n\n        # helper variable for sparse dropout\n        self.num_features_nonzero = placeholders['num_features_nonzero']\n\n        with tf.variable_scope(self.name + '_vars'):\n            self.vars['weights'] = glorot([input_dim, output_dim],\n                                          name='weights')\n            if self.bias:\n                self.vars['bias'] = zeros([output_dim], name='bias')\n\n        if self.logging:\n            self._log_vars()\n\n    def _call(self, inputs):\n        x = inputs\n\n        # dropout\n        if self.sparse_inputs:\n            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n        else:\n            x = tf.nn.dropout(x, 1-self.dropout)\n\n        # transform\n        output = dot(x, self.vars['weights'], sparse=self.sparse_inputs)\n\n        # bias\n        if self.bias:\n            output += self.vars['bias']\n\n        return self.act(output)\n\n\nclass GraphConvolution(Layer):\n    \"\"\"Graph convolution layer.\"\"\"\n    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,\n                 sparse_inputs=False, act=tf.nn.relu, bias=False,\n                 featureless=False, **kwargs):\n        super(GraphConvolution, self).__init__(**kwargs)\n\n        if dropout:\n            self.dropout = placeholders['dropout']\n        else:\n            self.dropout = 0.\n\n        self.act = act\n        self.support = placeholders['support']\n        self.sparse_inputs = sparse_inputs\n        self.featureless = featureless\n        self.bias = bias\n\n        # helper variable for sparse dropout\n        self.num_features_nonzero = placeholders['num_features_nonzero']\n\n        with tf.variable_scope(self.name + '_vars'):\n            for i in range(len(self.support)):\n                self.vars['weights_' + str(i)] = glorot([input_dim, output_dim],\n                                                        name='weights_' + str(i))\n            if self.bias:\n                self.vars['bias'] = zeros([output_dim], name='bias')\n\n        if self.logging:\n            self._log_vars()\n\n    def _call(self, inputs):\n        x = inputs\n\n        # dropout\n        if self.sparse_inputs:\n            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n        else:\n            x = tf.nn.dropout(x, 1-self.dropout)\n\n        # convolve\n        supports = list()\n        for i in range(len(self.support)):\n            if not self.featureless:\n                pre_sup = dot(x, self.vars['weights_' + str(i)],\n                              sparse=self.sparse_inputs)\n            else:\n                pre_sup = self.vars['weights_' + str(i)]\n            support = dot(self.support[i], pre_sup, sparse=True)\n            supports.append(support)\n        output = tf.add_n(supports)\n\n        # bias\n        if self.bias:\n            output += self.vars['bias']\n        self.embedding = output #output\n        return self.act(output)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T15:14:55.029284Z","iopub.execute_input":"2023-01-18T15:14:55.029635Z","iopub.status.idle":"2023-01-18T15:14:55.055796Z","shell.execute_reply.started":"2023-01-18T15:14:55.029605Z","shell.execute_reply":"2023-01-18T15:14:55.054786Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Models","metadata":{}},{"cell_type":"code","source":"flags = tf.app.flags\nFLAGS = flags.FLAGS\n\n\nclass Model(object):\n    def __init__(self, **kwargs):\n        allowed_kwargs = {'name', 'logging'}\n        for kwarg in kwargs.keys():\n            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n        name = kwargs.get('name')\n        if not name:\n            name = self.__class__.__name__.lower()\n        self.name = name\n\n        logging = kwargs.get('logging', False)\n        self.logging = logging\n\n        self.vars = {}\n        self.placeholders = {}\n\n        self.layers = []\n        self.activations = []\n\n        self.inputs = None\n        self.outputs = None\n\n        self.loss = 0\n        self.accuracy = 0\n        self.optimizer = None\n        self.opt_op = None\n\n    def _build(self):\n        raise NotImplementedError\n\n    def build(self):\n        \"\"\" Wrapper for _build() \"\"\"\n        with tf.variable_scope(self.name):\n            self._build()\n\n        # Build sequential layer model\n        self.activations.append(self.inputs)\n        for layer in self.layers:\n            hidden = layer(self.activations[-1])\n            self.activations.append(hidden)\n        self.outputs = self.activations[-1]\n\n        # Store model variables for easy access\n        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n        self.vars = {var.name: var for var in variables}\n\n        # Build metrics\n        self._loss()\n        self._accuracy()\n\n        self.opt_op = self.optimizer.minimize(self.loss)\n\n    def predict(self):\n        pass\n\n    def _loss(self):\n        raise NotImplementedError\n\n    def _accuracy(self):\n        raise NotImplementedError\n\n    def save(self, sess=None):\n        if not sess:\n            raise AttributeError(\"TensorFlow session not provided.\")\n        saver = tf.train.Saver(self.vars)\n        save_path = saver.save(sess, \"tmp/%s.ckpt\" % self.name)\n        print(\"Model saved in file: %s\" % save_path)\n\n    def load(self, sess=None):\n        if not sess:\n            raise AttributeError(\"TensorFlow session not provided.\")\n        saver = tf.train.Saver(self.vars)\n        save_path = \"tmp/%s.ckpt\" % self.name\n        saver.restore(sess, save_path)\n        print(\"Model restored from file: %s\" % save_path)\n\n\nclass MLP(Model):\n    def __init__(self, placeholders, input_dim, **kwargs):\n        super(MLP, self).__init__(**kwargs)\n\n        self.inputs = placeholders['features']\n        self.input_dim = input_dim\n        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n        self.placeholders = placeholders\n\n        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n\n        self.build()\n\n    def _loss(self):\n        # Weight decay loss\n        for var in self.layers[0].vars.values():\n            self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var)\n\n        # Cross entropy error\n        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n                                                  self.placeholders['labels_mask'])\n\n    def _accuracy(self):\n        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n                                        self.placeholders['labels_mask'])\n\n    def _build(self):\n        self.layers.append(Dense(input_dim=self.input_dim,\n                                 output_dim=FLAGS.hidden1,\n                                 placeholders=self.placeholders,\n                                 act=tf.nn.relu,\n                                 dropout=True,\n                                 sparse_inputs=True,\n                                 logging=self.logging))\n\n        self.layers.append(Dense(input_dim=FLAGS.hidden1,\n                                 output_dim=self.output_dim,\n                                 placeholders=self.placeholders,\n                                 act=lambda x: x,\n                                 dropout=True,\n                                 logging=self.logging))\n\n    def predict(self):\n        return tf.nn.softmax(self.outputs)\n\n\nclass GCN(Model):\n    def __init__(self, placeholders, input_dim, **kwargs):\n        super(GCN, self).__init__(**kwargs)\n\n        self.inputs = placeholders['features']\n        self.input_dim = input_dim\n        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n        self.placeholders = placeholders\n\n        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n\n        self.build()\n\n    def _loss(self):\n        # Weight decay loss\n        for var in self.layers[0].vars.values():\n            self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var)\n\n        # Cross entropy error\n        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n                                                  self.placeholders['labels_mask'])\n\n    def _accuracy(self):\n        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n                                        self.placeholders['labels_mask'])\n        self.pred = tf.argmax(self.outputs, 1)\n        self.labels = tf.argmax(self.placeholders['labels'], 1)\n\n    def _build(self):\n\n        self.layers.append(GraphConvolution(input_dim=self.input_dim,\n                                            output_dim=FLAGS.hidden1,\n                                            placeholders=self.placeholders,\n                                            act=tf.nn.relu,\n                                            dropout=True,\n                                            featureless=True,\n                                            sparse_inputs=True,\n                                            logging=self.logging))\n\n        self.layers.append(GraphConvolution(input_dim=FLAGS.hidden1,\n                                            output_dim=self.output_dim,\n                                            placeholders=self.placeholders,\n                                            act=lambda x: x, #\n                                            dropout=True,\n                                            logging=self.logging))\n\n    def predict(self):\n        return tf.nn.softmax(self.outputs)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T15:24:28.693075Z","iopub.execute_input":"2023-01-18T15:24:28.693457Z","iopub.status.idle":"2023-01-18T15:24:28.720956Z","shell.execute_reply.started":"2023-01-18T15:24:28.693427Z","shell.execute_reply":"2023-01-18T15:24:28.719916Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"markdown","source":"### RUN ONCE!!","metadata":{}},{"cell_type":"code","source":"import time\nfrom sklearn import metrics\nimport random\nimport os\nimport sys\n\ntf.compat.v1.flags.DEFINE_string('f','','')\ntf.compat.v1.disable_eager_execution()\ndataset = 'imdb'\n\n# Set random seed\nseed = random.randint(1, 200)\nnp.random.seed(seed)\ntf.set_random_seed(seed)\n\n# Settings\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n# 'cora', 'citeseer', 'pubmed'\nflags.DEFINE_string('dataset', dataset, 'Dataset string.')\n# 'gcn', 'gcn_cheby', 'dense'\nflags.DEFINE_string('model', 'gcn', 'Model string.')\nflags.DEFINE_float('learning_rate', 0.02, 'Initial learning rate.')\nflags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\nflags.DEFINE_integer('hidden1', 200, 'Number of units in hidden layer 1.')\nflags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\nflags.DEFINE_float('weight_decay', 0,\n                   'Weight for L2 loss on embedding matrix.')  # 5e-4\nflags.DEFINE_integer('early_stopping', 10,\n                     'Tolerance for early stopping (# of epochs).')\nflags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')","metadata":{"execution":{"iopub.status.busy":"2023-01-18T15:15:05.837888Z","iopub.execute_input":"2023-01-18T15:15:05.839043Z","iopub.status.idle":"2023-01-18T15:15:06.384396Z","shell.execute_reply.started":"2023-01-18T15:15:05.839001Z","shell.execute_reply":"2023-01-18T15:15:06.383444Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<absl.flags._flagvalues.FlagHolder at 0x7fe1d04b90d0>"},"metadata":{}}]},{"cell_type":"markdown","source":"Boleh run berkali2","metadata":{}},{"cell_type":"code","source":"# Load data\nadj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size = load_corpus(\n    FLAGS.dataset)\nprint(adj)\n# print(adj[0], adj[1])\nfeatures = sp.identity(features.shape[0])  # featureless\n\nprint(adj.shape)\nprint(features.shape)\n\n# Some preprocessing\nfeatures = preprocess_features(features)\nif FLAGS.model == 'gcn':\n    support = [preprocess_adj(adj)]\n    num_supports = 1\n    model_func = GCN\nelif FLAGS.model == 'gcn_cheby':\n    support = chebyshev_polynomials(adj, FLAGS.max_degree)\n    num_supports = 1 + FLAGS.max_degree\n    model_func = GCN\nelif FLAGS.model == 'dense':\n    support = [preprocess_adj(adj)]  # Not used\n    num_supports = 1\n    model_func = MLP\nelse:\n    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\n\n# Define placeholders\nplaceholders = {\n    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n    'labels_mask': tf.placeholder(tf.int32),\n    'dropout': tf.placeholder_with_default(0., shape=()),\n    # helper variable for sparse dropout\n    'num_features_nonzero': tf.placeholder(tf.int32)\n}\n\n# Create model\nprint(features[2][1])\nmodel = model_func(placeholders, input_dim=features[2][1], logging=True)\n\n# Initialize session\nsession_conf = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\nsess = tf.Session(config=session_conf)\n\n\n# Define model evaluation function\ndef evaluate(features, support, labels, mask, placeholders):\n    t_test = time.time()\n    feed_dict_val = construct_feed_dict(\n        features, support, labels, mask, placeholders)\n    outs_val = sess.run([model.loss, model.accuracy, model.pred, model.labels], feed_dict=feed_dict_val)\n    return outs_val[0], outs_val[1], outs_val[2], outs_val[3], (time.time() - t_test)\n\n\n# Init variables\nsess.run(tf.global_variables_initializer())\n\ncost_val = []\ncost_train = []\nacc_val = []\nacc_train = []\n\n# Train model\nfor epoch in range(FLAGS.epochs):\n\n    t = time.time()\n    # Construct feed dictionary\n    feed_dict = construct_feed_dict(\n        features, support, y_train, train_mask, placeholders)\n    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n\n    # Training step\n    outs = sess.run([model.opt_op, model.loss, model.accuracy,\n                     model.layers[0].embedding], feed_dict=feed_dict)\n\n    # Validation\n    cost, acc, pred, labels, duration = evaluate(\n        features, support, y_val, val_mask, placeholders)\n    \n    cost_val.append(cost)\n    acc_val.append(acc)\n    cost_train.append(outs[1])\n    acc_train.append(outs[2])\n    \n    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n          \"train_acc=\", \"{:.5f}\".format(\n              outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n\n    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n        print(\"Early stopping...\")\n        break\n\nprint(\"Optimization Finished!\")\nhistory = {\n    'accuracy': acc_train,\n    'loss': cost_train,\n    'val_accuracy': acc_val,\n    'val_loss': cost_val\n}\nprint('History built!')\n\n# Testing\ntest_cost, test_acc, pred, labels, test_duration = evaluate(\n    features, support, y_test, test_mask, placeholders)\nprint(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\n\ntest_pred = []\ntest_labels = []\nprint(len(test_mask))\nfor i in range(len(test_mask)):\n    if test_mask[i]:\n        test_pred.append(pred[i])\n        test_labels.append(labels[i])\n\nprint(\"Test Precision, Recall and F1-Score...\")\nprint(metrics.classification_report(test_labels, test_pred, digits=4))\nprint(\"Macro average Test Precision, Recall and F1-Score...\")\nprint(metrics.precision_recall_fscore_support(test_labels, test_pred, average='macro'))\nprint(\"Micro average Test Precision, Recall and F1-Score...\")\nprint(metrics.precision_recall_fscore_support(test_labels, test_pred, average='micro'))\n\n# doc and word embeddings\nprint('embeddings:')\nword_embeddings = outs[3][train_size: adj.shape[0] - test_size]\ntrain_doc_embeddings = outs[3][:train_size]  # include val docs\ntest_doc_embeddings = outs[3][adj.shape[0] - test_size:]\n\nprint(len(word_embeddings), len(train_doc_embeddings),\n      len(test_doc_embeddings))\nprint(word_embeddings)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T15:25:35.241571Z","iopub.execute_input":"2023-01-18T15:25:35.241953Z","iopub.status.idle":"2023-01-18T16:03:59.321811Z","shell.execute_reply.started":"2023-01-18T15:25:35.241921Z","shell.execute_reply":"2023-01-18T16:03:59.319897Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"(36000, 1000) (36000, 2) (10000, 1000) (10000, 2) (79522, 1000) (79522, 2)\n89522\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:22: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","output_type":"stream"},{"name":"stdout","text":"  (0, 40147)\t8.517193191416238\n  (0, 40380)\t6.12843040218114\n  (0, 40536)\t3.5805633106304775\n  (0, 41626)\t3.3220165837877143\n  (0, 41980)\t2.6861908667493166\n  (0, 42604)\t2.4039535873823343\n  (0, 44138)\t5.057726901630106\n  (0, 44366)\t2.7844993732656165\n  (0, 44979)\t12.754254055839933\n  (0, 45642)\t6.830794237846009\n  (0, 45835)\t1.4765023258423722\n  (0, 45989)\t1.5229850247039014\n  (0, 47935)\t4.463670623714392\n  (0, 49329)\t3.955974892957329\n  (0, 50127)\t2.4632234389568586\n  (0, 63740)\t7.418580902748128\n  (0, 65603)\t2.2557015070951953\n  (0, 66695)\t1.6074399097714274\n  (0, 67612)\t2.346745988779815\n  (0, 68433)\t2.6445121802982223\n  (0, 68569)\t3.5957534761244525\n  (0, 69801)\t2.647896278282463\n  (0, 70128)\t7.3232707229438025\n  (0, 70775)\t2.4112845099173867\n  (0, 71521)\t2.424526763799288\n  :\t:\n  (89521, 76565)\t3.0614448169193724\n  (89521, 76686)\t6.502290170873972\n  (89521, 76738)\t11.003316581132134\n  (89521, 76906)\t5.381698975487088\n  (89521, 76928)\t4.753670194306535\n  (89521, 76990)\t6.927957986299656\n  (89521, 77002)\t6.676643558018751\n  (89521, 77059)\t5.1672891041416324\n  (89521, 77184)\t2.4015558268588335\n  (89521, 77204)\t6.849486370858162\n  (89521, 77224)\t6.12843040218114\n  (89521, 77255)\t11.209685053602595\n  (89521, 77394)\t3.2990018693474856\n  (89521, 77447)\t15.041048975325516\n  (89521, 77636)\t3.9738984091462335\n  (89521, 77896)\t6.708904420236972\n  (89521, 77925)\t2.9565115604007097\n  (89521, 78180)\t2.158657924187403\n  (89521, 78346)\t1.814864342943152\n  (89521, 79011)\t4.871743295229638\n  (89521, 79072)\t1.9980459034758418\n  (89521, 79357)\t2.557735440443342\n  (89521, 79449)\t6.660895201050611\n  (89521, 79462)\t5.054587181625439\n  (89521, 79505)\t1.1767096525504572\n(89522, 89522)\n(89522, 89522)\n89522\nTensor(\"graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0\", shape=(None, 2), dtype=float32)\n","output_type":"stream"},{"name":"stderr","text":"2023-01-18 15:26:02.080644: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-01-18 15:26:02.232374: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n2023-01-18 15:26:02.232593: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 9ca7275c738b\n2023-01-18 15:26:02.232654: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 9ca7275c738b\n2023-01-18 15:26:02.232971: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.82.1\n2023-01-18 15:26:02.234039: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.82.1\n2023-01-18 15:26:02.234118: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.82.1\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 0001 train_loss= 0.69315 train_acc= 0.50289 val_loss= 0.69284 val_acc= 0.75225 time= 35.88951\nEpoch: 0002 train_loss= 0.69283 train_acc= 0.74330 val_loss= 0.69167 val_acc= 0.71475 time= 35.83645\nEpoch: 0003 train_loss= 0.69165 train_acc= 0.70550 val_loss= 0.68988 val_acc= 0.74700 time= 34.78651\nEpoch: 0004 train_loss= 0.68985 train_acc= 0.73217 val_loss= 0.68713 val_acc= 0.76275 time= 35.52728\nEpoch: 0005 train_loss= 0.68710 train_acc= 0.75475 val_loss= 0.68330 val_acc= 0.77250 time= 35.17049\nEpoch: 0006 train_loss= 0.68315 train_acc= 0.76744 val_loss= 0.67822 val_acc= 0.78125 time= 35.50251\nEpoch: 0007 train_loss= 0.67793 train_acc= 0.77883 val_loss= 0.67173 val_acc= 0.78275 time= 34.60627\nEpoch: 0008 train_loss= 0.67138 train_acc= 0.78475 val_loss= 0.66369 val_acc= 0.78875 time= 34.68132\nEpoch: 0009 train_loss= 0.66329 train_acc= 0.79055 val_loss= 0.65403 val_acc= 0.79400 time= 35.77764\nEpoch: 0010 train_loss= 0.65306 train_acc= 0.79730 val_loss= 0.64269 val_acc= 0.79400 time= 34.38897\nEpoch: 0011 train_loss= 0.64167 train_acc= 0.79894 val_loss= 0.62971 val_acc= 0.79400 time= 35.41272\nEpoch: 0012 train_loss= 0.62861 train_acc= 0.80294 val_loss= 0.61517 val_acc= 0.79725 time= 35.31460\nEpoch: 0013 train_loss= 0.61358 train_acc= 0.80600 val_loss= 0.59924 val_acc= 0.79950 time= 35.47142\nEpoch: 0014 train_loss= 0.59767 train_acc= 0.80805 val_loss= 0.58219 val_acc= 0.79950 time= 35.34925\nEpoch: 0015 train_loss= 0.58016 train_acc= 0.80883 val_loss= 0.56427 val_acc= 0.80275 time= 34.80703\nEpoch: 0016 train_loss= 0.56138 train_acc= 0.81222 val_loss= 0.54577 val_acc= 0.80700 time= 35.60525\nEpoch: 0017 train_loss= 0.54293 train_acc= 0.81514 val_loss= 0.52700 val_acc= 0.80975 time= 35.07773\nEpoch: 0018 train_loss= 0.52378 train_acc= 0.81744 val_loss= 0.50834 val_acc= 0.81375 time= 35.26926\nEpoch: 0019 train_loss= 0.50420 train_acc= 0.82314 val_loss= 0.49003 val_acc= 0.81800 time= 35.51560\nEpoch: 0020 train_loss= 0.48559 train_acc= 0.82550 val_loss= 0.47220 val_acc= 0.82300 time= 35.75965\nEpoch: 0021 train_loss= 0.46634 train_acc= 0.83008 val_loss= 0.45494 val_acc= 0.82750 time= 34.99026\nEpoch: 0022 train_loss= 0.44937 train_acc= 0.83555 val_loss= 0.43853 val_acc= 0.83475 time= 35.48886\nEpoch: 0023 train_loss= 0.43087 train_acc= 0.84016 val_loss= 0.42316 val_acc= 0.83825 time= 36.01625\nEpoch: 0024 train_loss= 0.41676 train_acc= 0.84291 val_loss= 0.40780 val_acc= 0.84650 time= 35.51278\nEpoch: 0025 train_loss= 0.40143 train_acc= 0.84953 val_loss= 0.39360 val_acc= 0.84925 time= 34.91282\nEpoch: 0026 train_loss= 0.38456 train_acc= 0.85586 val_loss= 0.38052 val_acc= 0.85600 time= 35.44468\nEpoch: 0027 train_loss= 0.37122 train_acc= 0.86050 val_loss= 0.36741 val_acc= 0.85800 time= 35.49889\nEpoch: 0028 train_loss= 0.35647 train_acc= 0.86514 val_loss= 0.35609 val_acc= 0.86350 time= 35.38780\nEpoch: 0029 train_loss= 0.34454 train_acc= 0.86875 val_loss= 0.34477 val_acc= 0.86725 time= 35.34250\nEpoch: 0030 train_loss= 0.33341 train_acc= 0.87389 val_loss= 0.33444 val_acc= 0.87325 time= 35.43155\nEpoch: 0031 train_loss= 0.32067 train_acc= 0.87822 val_loss= 0.32528 val_acc= 0.87700 time= 36.11373\nEpoch: 0032 train_loss= 0.30856 train_acc= 0.88314 val_loss= 0.31796 val_acc= 0.87625 time= 35.71319\nEpoch: 0033 train_loss= 0.30053 train_acc= 0.88789 val_loss= 0.30977 val_acc= 0.87950 time= 35.54546\nEpoch: 0034 train_loss= 0.29111 train_acc= 0.88905 val_loss= 0.30253 val_acc= 0.88100 time= 35.07484\nEpoch: 0035 train_loss= 0.28207 train_acc= 0.89305 val_loss= 0.29593 val_acc= 0.88600 time= 35.27532\nEpoch: 0036 train_loss= 0.27555 train_acc= 0.89669 val_loss= 0.28999 val_acc= 0.88675 time= 35.15889\nEpoch: 0037 train_loss= 0.26675 train_acc= 0.90055 val_loss= 0.28450 val_acc= 0.89125 time= 35.28811\nEpoch: 0038 train_loss= 0.26075 train_acc= 0.90178 val_loss= 0.27935 val_acc= 0.89350 time= 35.09064\nEpoch: 0039 train_loss= 0.25444 train_acc= 0.90414 val_loss= 0.27603 val_acc= 0.89450 time= 35.27185\nEpoch: 0040 train_loss= 0.24874 train_acc= 0.90508 val_loss= 0.27073 val_acc= 0.89725 time= 35.44303\nEpoch: 0041 train_loss= 0.24339 train_acc= 0.90819 val_loss= 0.26649 val_acc= 0.89900 time= 34.62711\nEpoch: 0042 train_loss= 0.23672 train_acc= 0.91102 val_loss= 0.26298 val_acc= 0.90075 time= 34.81903\nEpoch: 0043 train_loss= 0.23201 train_acc= 0.91230 val_loss= 0.25986 val_acc= 0.90225 time= 35.14029\nEpoch: 0044 train_loss= 0.22659 train_acc= 0.91544 val_loss= 0.25814 val_acc= 0.90350 time= 34.53894\nEpoch: 0045 train_loss= 0.22367 train_acc= 0.91569 val_loss= 0.25553 val_acc= 0.90350 time= 34.97968\nEpoch: 0046 train_loss= 0.21866 train_acc= 0.91758 val_loss= 0.25299 val_acc= 0.90500 time= 35.33206\nEpoch: 0047 train_loss= 0.21504 train_acc= 0.91916 val_loss= 0.25704 val_acc= 0.90000 time= 35.46725\nEpoch: 0048 train_loss= 0.21415 train_acc= 0.91827 val_loss= 0.25540 val_acc= 0.90075 time= 35.25157\nEpoch: 0049 train_loss= 0.21481 train_acc= 0.91827 val_loss= 0.25057 val_acc= 0.90400 time= 34.98278\nEpoch: 0050 train_loss= 0.20615 train_acc= 0.92158 val_loss= 0.24887 val_acc= 0.90450 time= 35.90331\nEpoch: 0051 train_loss= 0.20219 train_acc= 0.92400 val_loss= 0.24744 val_acc= 0.90625 time= 35.18918\nEpoch: 0052 train_loss= 0.20208 train_acc= 0.92466 val_loss= 0.24720 val_acc= 0.90425 time= 34.85412\nEpoch: 0053 train_loss= 0.19688 train_acc= 0.92691 val_loss= 0.24510 val_acc= 0.90525 time= 35.81960\nEpoch: 0054 train_loss= 0.19378 train_acc= 0.92750 val_loss= 0.24331 val_acc= 0.90625 time= 37.17860\nEpoch: 0055 train_loss= 0.19216 train_acc= 0.92936 val_loss= 0.24303 val_acc= 0.90500 time= 37.01097\nEpoch: 0056 train_loss= 0.18932 train_acc= 0.93039 val_loss= 0.24287 val_acc= 0.90575 time= 36.78534\nEpoch: 0057 train_loss= 0.18743 train_acc= 0.93197 val_loss= 0.24133 val_acc= 0.90775 time= 35.83468\nEpoch: 0058 train_loss= 0.18556 train_acc= 0.93222 val_loss= 0.24134 val_acc= 0.90750 time= 35.03006\nEpoch: 0059 train_loss= 0.18286 train_acc= 0.93355 val_loss= 0.24084 val_acc= 0.90825 time= 35.59887\nEpoch: 0060 train_loss= 0.18074 train_acc= 0.93439 val_loss= 0.23950 val_acc= 0.90600 time= 35.38344\nEpoch: 0061 train_loss= 0.17902 train_acc= 0.93491 val_loss= 0.23898 val_acc= 0.90775 time= 35.02543\nEpoch: 0062 train_loss= 0.17706 train_acc= 0.93622 val_loss= 0.23884 val_acc= 0.90925 time= 35.38451\nEpoch: 0063 train_loss= 0.17480 train_acc= 0.93686 val_loss= 0.23931 val_acc= 0.90600 time= 34.97524\nEpoch: 0064 train_loss= 0.17520 train_acc= 0.93544 val_loss= 0.24395 val_acc= 0.90750 time= 35.32859\nEarly stopping...\nOptimization Finished!\nHistory built!\nTest set results: cost= 0.23491 accuracy= 0.90810 time= 12.40606\n89522\nTest Precision, Recall and F1-Score...\n              precision    recall  f1-score   support\n\n           0     0.9280    0.8848    0.9059      5000\n           1     0.8899    0.9314    0.9102      5000\n\n    accuracy                         0.9081     10000\n   macro avg     0.9090    0.9081    0.9081     10000\nweighted avg     0.9090    0.9081    0.9081     10000\n\nMacro average Test Precision, Recall and F1-Score...\n(0.9089881422902719, 0.9081, 0.9080500813086416, None)\nMicro average Test Precision, Recall and F1-Score...\n(0.9081, 0.9081, 0.9081, None)\nembeddings:\n39522 40000 10000\n[[0.69700634 0.6774511  0.6886482  ... 0.7580177  0.58880466 0.59696203]\n [0.14457749 0.1408374  0.14905302 ... 0.14250956 0.12930419 0.13605814]\n [0.24775983 0.03063134 0.02468283 ... 0.26643598 0.21959858 0.23135762]\n ...\n [0.72576916 0.04609124 0.04170578 ... 0.7921733  0.6322932  0.64268136]\n [0.10233567 0.0564844  0.05655724 ... 0.11069252 0.08910309 0.09125201]\n [0.07096861 0.10304004 0.10193981 ... 0.08338892 0.05984791 0.06388265]]\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (12,6))\n\nax[0].plot(history['accuracy'], '-', label = 'Training Accuracy')\nax[0].plot(history['val_accuracy'], '-', label = 'Validation Accuracy')\nax[0].set_xlabel('Epochs')\nax[0].set_ylabel('Accuracy')\nax[0].set_title('Epochs & Training Accuracy', fontsize=20)\nax[0].legend(loc='best')\n\nax[1].plot(history['loss'], '-', label = 'Training loss')\nax[1].plot(history['val_loss'], '-', label = 'Validation loss')\nax[1].set_xlabel('Epochs')\nax[1].set_ylabel('loss')\nax[1].set_title('Epochs & Training loss', fontsize=20)\nax[1].legend(loc='best')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-18T16:04:30.132971Z","iopub.execute_input":"2023-01-18T16:04:30.133381Z","iopub.status.idle":"2023-01-18T16:04:30.502218Z","shell.execute_reply.started":"2023-01-18T16:04:30.133347Z","shell.execute_reply":"2023-01-18T16:04:30.501168Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 864x432 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"f = open('/kaggle/input/imdb-dataset-of-50k-movie-reviews-clean/data/data/' + dataset + '_vocab.corpus.txt', 'r')\nwords = f.readlines()\nf.close()\n\nvocab_size = len(words)\nword_vectors = []\nfor i in range(vocab_size):\n    word = words[i].strip()\n    word_vector = word_embeddings[i]\n    word_vector_str = ' '.join([str(x) for x in word_vector])\n    word_vectors.append(word + ' ' + word_vector_str)\n\nword_embeddings_str = '\\n'.join(word_vectors)\nf = open(dataset + '_word_vectors.txt', 'w')\nf.write(word_embeddings_str)\nf.close()\n\ndoc_vectors = []\ndoc_id = 0\nfor i in range(train_size):\n    doc_vector = train_doc_embeddings[i]\n    doc_vector_str = ' '.join([str(x) for x in doc_vector])\n    doc_vectors.append('doc_' + str(doc_id) + ' ' + doc_vector_str)\n    doc_id += 1\n\nfor i in range(test_size):\n    doc_vector = test_doc_embeddings[i]\n    doc_vector_str = ' '.join([str(x) for x in doc_vector])\n    doc_vectors.append('doc_' + str(doc_id) + ' ' + doc_vector_str)\n    doc_id += 1\n\ndoc_embeddings_str = '\\n'.join(doc_vectors)\nf = open(dataset + '_doc_vectors.txt', 'w')\nf.write(doc_embeddings_str)\nf.close()","metadata":{"execution":{"iopub.status.busy":"2023-01-18T16:04:40.930937Z","iopub.execute_input":"2023-01-18T16:04:40.931330Z","iopub.status.idle":"2023-01-18T16:04:51.737214Z","shell.execute_reply.started":"2023-01-18T16:04:40.931296Z","shell.execute_reply":"2023-01-18T16:04:51.736265Z"},"trusted":true},"execution_count":11,"outputs":[]}]}